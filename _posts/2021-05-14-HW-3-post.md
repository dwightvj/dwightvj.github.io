---
layout: post
title: Blog Post 3
---

In this blog post I look at using the machine learning framework, Tensorflow to gauge if an article is "fake news" based on an article's text and title. Our data hs been split into training and testing sets, but we wil begin with looking at the training data below. 

```python
import numpy as np
import pandas as pd
import plotly.express as px 
import plotly.io as pio
import nltk
nltk.download('stopwords')
from nltk.corpus import stopwords
import re
import string
import tensorflow as tf
from tensorflow.keras import layers
from tensorflow.keras import losses
from tensorflow import keras
from tensorflow.keras.layers.experimental.preprocessing import TextVectorization
from tensorflow.keras.layers.experimental.preprocessing import StringLookup
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import LabelEncoder
```

    [nltk_data] Downloading package stopwords to /root/nltk_data...
    [nltk_data]   Package stopwords is already up-to-date!
    


```python
train_url = "https://github.com/PhilChodrow/PIC16b/blob/master/datasets/fake_news_train.csv?raw=true"
df = pd.read_csv(train_url)
df.head()
```

<div>
<style scoped>
    .dataframe tbody tr th:only-of-type {
        vertical-align: middle;
    }

    .dataframe tbody tr th {
        vertical-align: top;
    }

    .dataframe thead th {
        text-align: right;
    }
</style>
<table border="1" class="dataframe">
  <thead>
    <tr style="text-align: right;">
      <th></th>
      <th>Unnamed: 0</th>
      <th>title</th>
      <th>text</th>
      <th>fake</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <th>0</th>
      <td>17366</td>
      <td>Merkel: Strong result for Austria's FPO 'big c...</td>
      <td>German Chancellor Angela Merkel said on Monday...</td>
      <td>0</td>
    </tr>
    <tr>
      <th>1</th>
      <td>5634</td>
      <td>Trump says Pence will lead voter fraud panel</td>
      <td>WEST PALM BEACH, Fla.President Donald Trump sa...</td>
      <td>0</td>
    </tr>
    <tr>
      <th>2</th>
      <td>17487</td>
      <td>JUST IN: SUSPECTED LEAKER and “Close Confidant...</td>
      <td>On December 5, 2017, Circa s Sara Carter warne...</td>
      <td>1</td>
    </tr>
    <tr>
      <th>3</th>
      <td>12217</td>
      <td>Thyssenkrupp has offered help to Argentina ove...</td>
      <td>Germany s Thyssenkrupp, has offered assistance...</td>
      <td>0</td>
    </tr>
    <tr>
      <th>4</th>
      <td>5535</td>
      <td>Trump say appeals court decision on travel ban...</td>
      <td>President Donald Trump on Thursday called the ...</td>
      <td>0</td>
    </tr>
  </tbody>
</table>
</div>

# Make a Dataset

Write a function called make_dataset. This function should do two things:

Remove stopwords from the article text and title. A stopword is a word that is usually considered to be uninformative, such as “the,” “and,” or “but.” You may find this StackOverFlow thread to be helpful.
Construct and return a tf.data.Dataset with two inputs and one output. The input should be of the form (title, text), and the output should consist only of the fake column. You may find it helpful to consult these lecture notes or this tutorial for reference on how to construct and use Datasets with multiple inputs.
Call the function make_dataset on your training dataframe to produce a Dataset. You may wish to batch your Dataset prior to returning it, which can be done like this: my_data_set.batch(100). Batching causes your model to train on chunks of data rather than individual rows. This can sometimes reduce accuracy, but can also greatly increase the speed of training. Finding a balance is key. I found batches of 100 rows to work well.

Validation Data
After you’ve constructed your primary Dataset, split of 20% of it to use for validation.



```python
stop = stopwords.words("english")

def make_dataset(df):
  for i in ["text", "title"]:
    df[i] = df[i].str.lower().str.split()
    df[i] = df[i].apply(lambda x: [item for item in x if item not in stop]).apply(" ".join)

  tf_data = tf.data.Dataset.from_tensor_slices(
      (
        {"title" :  df[["title"]],"text" : df[["text"]]},
        {"fake" : df[["fake"]]}
      )
  )

  return tf_data
```


```python
data = make_dataset(df)
```


```python
data = data.shuffle(buffer_size = len(data))

train_size = int(0.8*len(data))
val_size   = int(0.2*len(data))
train = data.take(train_size).batch(20)
val   = data.skip(train_size).take(val_size).batch(20)
```


```python
def standardization(input_data):
    lowercase = tf.strings.lower(input_data)
    no_punctuation = tf.strings.regex_replace(lowercase,
                                  '[%s]' % re.escape(string.punctuation),'')
    return no_punctuation
```


```python
size_vocab = 2000
vectorize_layer = TextVectorization(
    standardize            = standardization,
    max_tokens             = size_vocab,
    output_mode            = "int",
    output_sequence_length = 500)
```


```python
vectorize_layer.adapt(train.map(lambda x, y: x["text"]))
vectorize_layer.adapt(train.map(lambda x, y: x["title"]))
```


```python
text_input = keras.Input(
    shape = (1,),
    name = "text",
    dtype = "string"
)
```


```python
title_input = keras.Input(
    shape = (1,),
    name = "title",
    dtype = "string"
)
```


```python
shared_layer = layers.Embedding(size_vocab, 10, name = "embedding")
```

## Model 1


```python
title_features = vectorize_layer(title_input)
title_features = shared_layer(title_features)
title_features = layers.Dropout(0.2)(title_features)
title_features = layers.GlobalAveragePooling1D()(title_features)
title_features = layers.Dropout(0.2)(title_features)
title_features = layers.Dense(32, activation='relu')(title_features)
title_output = layers.Dense(2, name = "fake")(title_features)
```


```python
title_model = keras.Model(
    inputs = title_input,
    outputs = title_output
)

title_model.summary()
```

    Model: "model"
    _________________________________________________________________
    Layer (type)                 Output Shape              Param #   
    =================================================================
    title (InputLayer)           [(None, 1)]               0         
    _________________________________________________________________
    text_vectorization (TextVect (None, 500)               0         
    _________________________________________________________________
    embedding (Embedding)        (None, 500, 10)           20000     
    _________________________________________________________________
    dropout (Dropout)            (None, 500, 10)           0         
    _________________________________________________________________
    global_average_pooling1d (Gl (None, 10)                0         
    _________________________________________________________________
    dropout_1 (Dropout)          (None, 10)                0         
    _________________________________________________________________
    dense (Dense)                (None, 32)                352       
    _________________________________________________________________
    fake (Dense)                 (None, 2)                 66        
    =================================================================
    Total params: 20,418
    Trainable params: 20,418
    Non-trainable params: 0
    _________________________________________________________________
    


```python
keras.utils.plot_model(title_model)
```




    
![png](/images/output_14_0.png)
    




```python
title_model.compile(optimizer = "adam",
              loss = losses.SparseCategoricalCrossentropy(from_logits=True),
              metrics=['accuracy']
)
```


```python
history = title_model.fit(train, 
                    validation_data=val,
                    epochs = 50, 
                    verbose = 2)
```

    Epoch 1/50
    

    /usr/local/lib/python3.7/dist-packages/tensorflow/python/keras/engine/functional.py:595: UserWarning:
    
    Input dict contained keys ['text'] which did not match any model input. They will be ignored by the model.
    
    

    898/898 - 5s - loss: 0.6743 - accuracy: 0.5790 - val_loss: 0.5837 - val_accuracy: 0.8323
    Epoch 2/50
    898/898 - 4s - loss: 0.3986 - accuracy: 0.8431 - val_loss: 0.2866 - val_accuracy: 0.8830
    Epoch 3/50
    898/898 - 4s - loss: 0.2438 - accuracy: 0.9015 - val_loss: 0.1985 - val_accuracy: 0.9231
    Epoch 4/50
    898/898 - 4s - loss: 0.1921 - accuracy: 0.9244 - val_loss: 0.1736 - val_accuracy: 0.9289
    Epoch 5/50
    898/898 - 4s - loss: 0.1717 - accuracy: 0.9313 - val_loss: 0.1802 - val_accuracy: 0.9225
    Epoch 6/50
    898/898 - 4s - loss: 0.1578 - accuracy: 0.9373 - val_loss: 0.1398 - val_accuracy: 0.9461
    Epoch 7/50
    898/898 - 4s - loss: 0.1492 - accuracy: 0.9404 - val_loss: 0.1453 - val_accuracy: 0.9419
    Epoch 8/50
    898/898 - 4s - loss: 0.1406 - accuracy: 0.9450 - val_loss: 0.1211 - val_accuracy: 0.9519
    Epoch 9/50
    898/898 - 4s - loss: 0.1396 - accuracy: 0.9452 - val_loss: 0.1252 - val_accuracy: 0.9519
    Epoch 10/50
    898/898 - 4s - loss: 0.1370 - accuracy: 0.9474 - val_loss: 0.1122 - val_accuracy: 0.9550
    Epoch 11/50
    898/898 - 4s - loss: 0.1314 - accuracy: 0.9479 - val_loss: 0.1406 - val_accuracy: 0.9383
    Epoch 12/50
    898/898 - 4s - loss: 0.1247 - accuracy: 0.9506 - val_loss: 0.1130 - val_accuracy: 0.9575
    Epoch 13/50
    898/898 - 4s - loss: 0.1267 - accuracy: 0.9486 - val_loss: 0.1082 - val_accuracy: 0.9563
    Epoch 14/50
    898/898 - 4s - loss: 0.1213 - accuracy: 0.9525 - val_loss: 0.1422 - val_accuracy: 0.9401
    Epoch 15/50
    898/898 - 4s - loss: 0.1254 - accuracy: 0.9469 - val_loss: 0.1444 - val_accuracy: 0.9425
    Epoch 16/50
    898/898 - 4s - loss: 0.1188 - accuracy: 0.9524 - val_loss: 0.1101 - val_accuracy: 0.9497
    Epoch 17/50
    898/898 - 4s - loss: 0.1178 - accuracy: 0.9534 - val_loss: 0.1384 - val_accuracy: 0.9430
    Epoch 18/50
    898/898 - 4s - loss: 0.1173 - accuracy: 0.9536 - val_loss: 0.0882 - val_accuracy: 0.9635
    Epoch 19/50
    898/898 - 4s - loss: 0.1138 - accuracy: 0.9550 - val_loss: 0.1031 - val_accuracy: 0.9586
    Epoch 20/50
    898/898 - 4s - loss: 0.1135 - accuracy: 0.9546 - val_loss: 0.0814 - val_accuracy: 0.9710
    Epoch 21/50
    898/898 - 4s - loss: 0.1113 - accuracy: 0.9574 - val_loss: 0.0992 - val_accuracy: 0.9628
    Epoch 22/50
    898/898 - 4s - loss: 0.1130 - accuracy: 0.9553 - val_loss: 0.0928 - val_accuracy: 0.9652
    Epoch 23/50
    898/898 - 4s - loss: 0.1113 - accuracy: 0.9552 - val_loss: 0.1127 - val_accuracy: 0.9523
    Epoch 24/50
    898/898 - 4s - loss: 0.1123 - accuracy: 0.9550 - val_loss: 0.0898 - val_accuracy: 0.9666
    Epoch 25/50
    898/898 - 4s - loss: 0.1101 - accuracy: 0.9567 - val_loss: 0.1003 - val_accuracy: 0.9617
    Epoch 26/50
    898/898 - 4s - loss: 0.1124 - accuracy: 0.9550 - val_loss: 0.0934 - val_accuracy: 0.9641
    Epoch 27/50
    898/898 - 4s - loss: 0.1092 - accuracy: 0.9560 - val_loss: 0.0843 - val_accuracy: 0.9664
    Epoch 28/50
    898/898 - 4s - loss: 0.1069 - accuracy: 0.9584 - val_loss: 0.1452 - val_accuracy: 0.9370
    Epoch 29/50
    898/898 - 4s - loss: 0.1076 - accuracy: 0.9572 - val_loss: 0.0912 - val_accuracy: 0.9646
    Epoch 30/50
    898/898 - 4s - loss: 0.1033 - accuracy: 0.9576 - val_loss: 0.0924 - val_accuracy: 0.9624
    Epoch 31/50
    898/898 - 4s - loss: 0.1047 - accuracy: 0.9597 - val_loss: 0.1095 - val_accuracy: 0.9557
    Epoch 32/50
    898/898 - 4s - loss: 0.1057 - accuracy: 0.9573 - val_loss: 0.1211 - val_accuracy: 0.9505
    Epoch 33/50
    898/898 - 4s - loss: 0.1045 - accuracy: 0.9596 - val_loss: 0.0866 - val_accuracy: 0.9673
    Epoch 34/50
    898/898 - 4s - loss: 0.1043 - accuracy: 0.9582 - val_loss: 0.0938 - val_accuracy: 0.9621
    Epoch 35/50
    898/898 - 4s - loss: 0.1011 - accuracy: 0.9601 - val_loss: 0.1228 - val_accuracy: 0.9514
    Epoch 36/50
    898/898 - 4s - loss: 0.1046 - accuracy: 0.9597 - val_loss: 0.0824 - val_accuracy: 0.9684
    Epoch 37/50
    898/898 - 4s - loss: 0.1037 - accuracy: 0.9585 - val_loss: 0.0961 - val_accuracy: 0.9590
    Epoch 38/50
    898/898 - 4s - loss: 0.1016 - accuracy: 0.9589 - val_loss: 0.0726 - val_accuracy: 0.9701
    Epoch 39/50
    898/898 - 4s - loss: 0.1014 - accuracy: 0.9607 - val_loss: 0.0915 - val_accuracy: 0.9641
    Epoch 40/50
    898/898 - 4s - loss: 0.0995 - accuracy: 0.9597 - val_loss: 0.0860 - val_accuracy: 0.9693
    Epoch 41/50
    898/898 - 4s - loss: 0.1047 - accuracy: 0.9579 - val_loss: 0.0770 - val_accuracy: 0.9724
    Epoch 42/50
    898/898 - 4s - loss: 0.0967 - accuracy: 0.9630 - val_loss: 0.1018 - val_accuracy: 0.9586
    Epoch 43/50
    898/898 - 4s - loss: 0.1030 - accuracy: 0.9594 - val_loss: 0.1006 - val_accuracy: 0.9568
    Epoch 44/50
    898/898 - 4s - loss: 0.1003 - accuracy: 0.9606 - val_loss: 0.0798 - val_accuracy: 0.9697
    Epoch 45/50
    898/898 - 4s - loss: 0.1006 - accuracy: 0.9596 - val_loss: 0.0953 - val_accuracy: 0.9601
    Epoch 46/50
    898/898 - 4s - loss: 0.1044 - accuracy: 0.9585 - val_loss: 0.0840 - val_accuracy: 0.9679
    Epoch 47/50
    898/898 - 4s - loss: 0.0996 - accuracy: 0.9606 - val_loss: 0.0750 - val_accuracy: 0.9724
    Epoch 48/50
    898/898 - 4s - loss: 0.0958 - accuracy: 0.9621 - val_loss: 0.0948 - val_accuracy: 0.9599
    Epoch 49/50
    898/898 - 4s - loss: 0.0985 - accuracy: 0.9603 - val_loss: 0.0905 - val_accuracy: 0.9630
    Epoch 50/50
    898/898 - 4s - loss: 0.1016 - accuracy: 0.9600 - val_loss: 0.0850 - val_accuracy: 0.9684
    


```python
from matplotlib import pyplot as plt
plt.plot(history.history["accuracy"])
plt.plot(history.history["val_accuracy"])
```



    
![png](/images/output_17_1.png)
    


## Model 2


```python
text_features = vectorize_layer(text_input)
text_features = shared_layer(text_features)
text_features = layers.Dropout(0.2)(text_features)
text_features = layers.GlobalAveragePooling1D()(text_features)
text_features = layers.Dropout(0.2)(text_features)
text_features = layers.Dense(32, activation='relu')(text_features)
text_output = layers.Dense(2, name = "fake")(text_features)
```


```python
text_model = keras.Model(
    inputs = text_input,
    outputs = text_output
)

text_model.summary()
```

    Model: "model_1"
    _________________________________________________________________
    Layer (type)                 Output Shape              Param #   
    =================================================================
    text (InputLayer)            [(None, 1)]               0         
    _________________________________________________________________
    text_vectorization (TextVect (None, 500)               0         
    _________________________________________________________________
    embedding (Embedding)        (None, 500, 10)           20000     
    _________________________________________________________________
    dropout_2 (Dropout)          (None, 500, 10)           0         
    _________________________________________________________________
    global_average_pooling1d_1 ( (None, 10)                0         
    _________________________________________________________________
    dropout_3 (Dropout)          (None, 10)                0         
    _________________________________________________________________
    dense_1 (Dense)              (None, 32)                352       
    _________________________________________________________________
    fake (Dense)                 (None, 2)                 66        
    =================================================================
    Total params: 20,418
    Trainable params: 20,418
    Non-trainable params: 0
    _________________________________________________________________
    


```python
text_model.compile(optimizer = "adam",
              loss = losses.SparseCategoricalCrossentropy(from_logits=True),
              metrics=['accuracy']
)
```


```python
history = text_model.fit(train, 
                    validation_data=val,
                    epochs = 50,
                    verbose = 2)
```

    Epoch 1/50
    

    /usr/local/lib/python3.7/dist-packages/tensorflow/python/keras/engine/functional.py:595: UserWarning:
    
    Input dict contained keys ['title'] which did not match any model input. They will be ignored by the model.
    
    

    898/898 - 6s - loss: 0.3504 - accuracy: 0.8534 - val_loss: 0.2202 - val_accuracy: 0.9252
    Epoch 2/50
    898/898 - 5s - loss: 0.1936 - accuracy: 0.9337 - val_loss: 0.1656 - val_accuracy: 0.9494
    Epoch 3/50
    898/898 - 5s - loss: 0.1601 - accuracy: 0.9470 - val_loss: 0.1422 - val_accuracy: 0.9537
    Epoch 4/50
    898/898 - 5s - loss: 0.1369 - accuracy: 0.9561 - val_loss: 0.1160 - val_accuracy: 0.9632
    Epoch 5/50
    898/898 - 5s - loss: 0.1209 - accuracy: 0.9619 - val_loss: 0.1051 - val_accuracy: 0.9675
    Epoch 6/50
    898/898 - 5s - loss: 0.1120 - accuracy: 0.9650 - val_loss: 0.1008 - val_accuracy: 0.9695
    Epoch 7/50
    898/898 - 5s - loss: 0.1064 - accuracy: 0.9661 - val_loss: 0.0993 - val_accuracy: 0.9559
    Epoch 8/50
    898/898 - 5s - loss: 0.0956 - accuracy: 0.9693 - val_loss: 0.0839 - val_accuracy: 0.9739
    Epoch 9/50
    898/898 - 5s - loss: 0.0907 - accuracy: 0.9717 - val_loss: 0.0674 - val_accuracy: 0.9804
    Epoch 10/50
    898/898 - 5s - loss: 0.0866 - accuracy: 0.9727 - val_loss: 0.0771 - val_accuracy: 0.9768
    Epoch 11/50
    898/898 - 5s - loss: 0.0851 - accuracy: 0.9742 - val_loss: 0.0757 - val_accuracy: 0.9753
    Epoch 12/50
    898/898 - 5s - loss: 0.0768 - accuracy: 0.9760 - val_loss: 0.0717 - val_accuracy: 0.9779
    Epoch 13/50
    898/898 - 5s - loss: 0.0760 - accuracy: 0.9777 - val_loss: 0.0777 - val_accuracy: 0.9773
    Epoch 14/50
    898/898 - 5s - loss: 0.0718 - accuracy: 0.9794 - val_loss: 0.0620 - val_accuracy: 0.9793
    Epoch 15/50
    898/898 - 5s - loss: 0.0678 - accuracy: 0.9793 - val_loss: 0.0594 - val_accuracy: 0.9817
    Epoch 16/50
    898/898 - 5s - loss: 0.0625 - accuracy: 0.9802 - val_loss: 0.0487 - val_accuracy: 0.9849
    Epoch 17/50
    898/898 - 5s - loss: 0.0636 - accuracy: 0.9810 - val_loss: 0.0526 - val_accuracy: 0.9844
    Epoch 18/50
    898/898 - 5s - loss: 0.0620 - accuracy: 0.9808 - val_loss: 0.0600 - val_accuracy: 0.9828
    Epoch 19/50
    898/898 - 5s - loss: 0.0600 - accuracy: 0.9813 - val_loss: 0.0462 - val_accuracy: 0.9869
    Epoch 20/50
    898/898 - 5s - loss: 0.0556 - accuracy: 0.9835 - val_loss: 0.0514 - val_accuracy: 0.9862
    Epoch 21/50
    898/898 - 5s - loss: 0.0541 - accuracy: 0.9840 - val_loss: 0.0533 - val_accuracy: 0.9851
    Epoch 22/50
    898/898 - 5s - loss: 0.0523 - accuracy: 0.9839 - val_loss: 0.0446 - val_accuracy: 0.9871
    Epoch 23/50
    898/898 - 5s - loss: 0.0511 - accuracy: 0.9846 - val_loss: 0.0399 - val_accuracy: 0.9893
    Epoch 24/50
    898/898 - 5s - loss: 0.0499 - accuracy: 0.9857 - val_loss: 0.0373 - val_accuracy: 0.9911
    Epoch 25/50
    898/898 - 5s - loss: 0.0474 - accuracy: 0.9849 - val_loss: 0.0333 - val_accuracy: 0.9911
    Epoch 26/50
    898/898 - 5s - loss: 0.0466 - accuracy: 0.9859 - val_loss: 0.0395 - val_accuracy: 0.9882
    Epoch 27/50
    898/898 - 5s - loss: 0.0444 - accuracy: 0.9868 - val_loss: 0.0381 - val_accuracy: 0.9909
    Epoch 28/50
    898/898 - 5s - loss: 0.0447 - accuracy: 0.9865 - val_loss: 0.0316 - val_accuracy: 0.9911
    Epoch 29/50
    898/898 - 5s - loss: 0.0417 - accuracy: 0.9874 - val_loss: 0.0359 - val_accuracy: 0.9906
    Epoch 30/50
    898/898 - 5s - loss: 0.0404 - accuracy: 0.9880 - val_loss: 0.0356 - val_accuracy: 0.9904
    Epoch 31/50
    898/898 - 5s - loss: 0.0392 - accuracy: 0.9879 - val_loss: 0.0391 - val_accuracy: 0.9877
    Epoch 32/50
    898/898 - 5s - loss: 0.0400 - accuracy: 0.9881 - val_loss: 0.0309 - val_accuracy: 0.9909
    Epoch 33/50
    898/898 - 5s - loss: 0.0373 - accuracy: 0.9886 - val_loss: 0.0309 - val_accuracy: 0.9935
    Epoch 34/50
    898/898 - 5s - loss: 0.0356 - accuracy: 0.9889 - val_loss: 0.0291 - val_accuracy: 0.9922
    Epoch 35/50
    898/898 - 5s - loss: 0.0353 - accuracy: 0.9891 - val_loss: 0.0270 - val_accuracy: 0.9922
    Epoch 36/50
    898/898 - 5s - loss: 0.0337 - accuracy: 0.9896 - val_loss: 0.0324 - val_accuracy: 0.9891
    Epoch 37/50
    898/898 - 5s - loss: 0.0333 - accuracy: 0.9890 - val_loss: 0.0296 - val_accuracy: 0.9904
    Epoch 38/50
    898/898 - 5s - loss: 0.0318 - accuracy: 0.9896 - val_loss: 0.0237 - val_accuracy: 0.9944
    Epoch 39/50
    898/898 - 5s - loss: 0.0330 - accuracy: 0.9895 - val_loss: 0.0252 - val_accuracy: 0.9922
    Epoch 40/50
    898/898 - 5s - loss: 0.0306 - accuracy: 0.9904 - val_loss: 0.0223 - val_accuracy: 0.9924
    Epoch 41/50
    898/898 - 5s - loss: 0.0299 - accuracy: 0.9908 - val_loss: 0.0301 - val_accuracy: 0.9915
    Epoch 42/50
    898/898 - 5s - loss: 0.0306 - accuracy: 0.9894 - val_loss: 0.0205 - val_accuracy: 0.9944
    Epoch 43/50
    898/898 - 5s - loss: 0.0274 - accuracy: 0.9910 - val_loss: 0.0174 - val_accuracy: 0.9955
    Epoch 44/50
    898/898 - 5s - loss: 0.0284 - accuracy: 0.9905 - val_loss: 0.0270 - val_accuracy: 0.9906
    Epoch 45/50
    898/898 - 5s - loss: 0.0265 - accuracy: 0.9913 - val_loss: 0.0180 - val_accuracy: 0.9933
    Epoch 46/50
    898/898 - 5s - loss: 0.0271 - accuracy: 0.9915 - val_loss: 0.0172 - val_accuracy: 0.9958
    Epoch 47/50
    898/898 - 5s - loss: 0.0249 - accuracy: 0.9924 - val_loss: 0.0210 - val_accuracy: 0.9942
    Epoch 48/50
    898/898 - 5s - loss: 0.0259 - accuracy: 0.9921 - val_loss: 0.0159 - val_accuracy: 0.9958
    Epoch 49/50
    898/898 - 5s - loss: 0.0261 - accuracy: 0.9919 - val_loss: 0.0191 - val_accuracy: 0.9947
    Epoch 50/50
    898/898 - 5s - loss: 0.0241 - accuracy: 0.9909 - val_loss: 0.0155 - val_accuracy: 0.9958
    


```python
plt.plot(history.history["accuracy"])
plt.plot(history.history["val_accuracy"])
```





    
![png](/images/output_23_1.png)
    


## Model 3


```python
main = layers.concatenate([title_features, text_features], axis = 1)
```


```python
main = layers.Dense(32, activation='relu')(main)
output = layers.Dense(2, name = "fake")(main)
```


```python
final_model = keras.Model(
    inputs = [title_input, text_input],
    outputs = output
)

final_model.summary()
```

    Model: "model_2"
    __________________________________________________________________________________________________
    Layer (type)                    Output Shape         Param #     Connected to                     
    ==================================================================================================
    title (InputLayer)              [(None, 1)]          0                                            
    __________________________________________________________________________________________________
    text (InputLayer)               [(None, 1)]          0                                            
    __________________________________________________________________________________________________
    text_vectorization (TextVectori (None, 500)          0           title[0][0]                      
                                                                     text[0][0]                       
    __________________________________________________________________________________________________
    embedding (Embedding)           (None, 500, 10)      20000       text_vectorization[0][0]         
                                                                     text_vectorization[1][0]         
    __________________________________________________________________________________________________
    dropout (Dropout)               (None, 500, 10)      0           embedding[0][0]                  
    __________________________________________________________________________________________________
    dropout_2 (Dropout)             (None, 500, 10)      0           embedding[1][0]                  
    __________________________________________________________________________________________________
    global_average_pooling1d (Globa (None, 10)           0           dropout[0][0]                    
    __________________________________________________________________________________________________
    global_average_pooling1d_1 (Glo (None, 10)           0           dropout_2[0][0]                  
    __________________________________________________________________________________________________
    dropout_1 (Dropout)             (None, 10)           0           global_average_pooling1d[0][0]   
    __________________________________________________________________________________________________
    dropout_3 (Dropout)             (None, 10)           0           global_average_pooling1d_1[0][0] 
    __________________________________________________________________________________________________
    dense (Dense)                   (None, 32)           352         dropout_1[0][0]                  
    __________________________________________________________________________________________________
    dense_1 (Dense)                 (None, 32)           352         dropout_3[0][0]                  
    __________________________________________________________________________________________________
    concatenate (Concatenate)       (None, 64)           0           dense[0][0]                      
                                                                     dense_1[0][0]                    
    __________________________________________________________________________________________________
    dense_2 (Dense)                 (None, 32)           2080        concatenate[0][0]                
    __________________________________________________________________________________________________
    fake (Dense)                    (None, 2)            66          dense_2[0][0]                    
    ==================================================================================================
    Total params: 22,850
    Trainable params: 22,850
    Non-trainable params: 0
    __________________________________________________________________________________________________
    


```python
final_model.compile(optimizer = "adam",
              loss = losses.SparseCategoricalCrossentropy(from_logits=True),
              metrics=['accuracy']
)
```


```python
history = final_model.fit(train, 
                    validation_data=val,
                    epochs = 50,
                    verbose = 2)
```

    Epoch 1/50
    898/898 - 7s - loss: 0.0557 - accuracy: 0.9852 - val_loss: 0.0167 - val_accuracy: 0.9944
    Epoch 2/50
    898/898 - 6s - loss: 0.0209 - accuracy: 0.9923 - val_loss: 0.0142 - val_accuracy: 0.9951
    Epoch 3/50
    898/898 - 6s - loss: 0.0175 - accuracy: 0.9936 - val_loss: 0.0134 - val_accuracy: 0.9947
    Epoch 4/50
    898/898 - 6s - loss: 0.0152 - accuracy: 0.9944 - val_loss: 0.0096 - val_accuracy: 0.9960
    Epoch 5/50
    898/898 - 6s - loss: 0.0160 - accuracy: 0.9942 - val_loss: 0.0075 - val_accuracy: 0.9973
    Epoch 6/50
    898/898 - 6s - loss: 0.0128 - accuracy: 0.9956 - val_loss: 0.0204 - val_accuracy: 0.9920
    Epoch 7/50
    898/898 - 6s - loss: 0.0128 - accuracy: 0.9954 - val_loss: 0.0054 - val_accuracy: 0.9987
    Epoch 8/50
    898/898 - 6s - loss: 0.0126 - accuracy: 0.9951 - val_loss: 0.0039 - val_accuracy: 0.9989
    Epoch 9/50
    898/898 - 6s - loss: 0.0121 - accuracy: 0.9958 - val_loss: 0.0027 - val_accuracy: 0.9993
    Epoch 10/50
    898/898 - 6s - loss: 0.0125 - accuracy: 0.9955 - val_loss: 0.0058 - val_accuracy: 0.9989
    Epoch 11/50
    898/898 - 6s - loss: 0.0116 - accuracy: 0.9958 - val_loss: 0.0045 - val_accuracy: 0.9982
    Epoch 12/50
    898/898 - 6s - loss: 0.0103 - accuracy: 0.9962 - val_loss: 0.0031 - val_accuracy: 0.9991
    Epoch 13/50
    898/898 - 6s - loss: 0.0110 - accuracy: 0.9961 - val_loss: 0.0025 - val_accuracy: 0.9996
    Epoch 14/50
    898/898 - 6s - loss: 0.0117 - accuracy: 0.9955 - val_loss: 0.0033 - val_accuracy: 0.9993
    Epoch 15/50
    898/898 - 6s - loss: 0.0107 - accuracy: 0.9962 - val_loss: 0.0036 - val_accuracy: 0.9987
    Epoch 16/50
    898/898 - 6s - loss: 0.0088 - accuracy: 0.9965 - val_loss: 0.0038 - val_accuracy: 0.9987
    Epoch 17/50
    898/898 - 6s - loss: 0.0104 - accuracy: 0.9961 - val_loss: 0.0039 - val_accuracy: 0.9982
    Epoch 18/50
    898/898 - 6s - loss: 0.0109 - accuracy: 0.9956 - val_loss: 0.0032 - val_accuracy: 0.9996
    Epoch 19/50
    898/898 - 6s - loss: 0.0078 - accuracy: 0.9972 - val_loss: 0.0046 - val_accuracy: 0.9982
    Epoch 20/50
    898/898 - 6s - loss: 0.0082 - accuracy: 0.9973 - val_loss: 0.0036 - val_accuracy: 0.9984
    Epoch 21/50
    898/898 - 6s - loss: 0.0097 - accuracy: 0.9965 - val_loss: 0.0043 - val_accuracy: 0.9980
    Epoch 22/50
    898/898 - 6s - loss: 0.0078 - accuracy: 0.9967 - val_loss: 0.0019 - val_accuracy: 0.9996
    Epoch 23/50
    898/898 - 6s - loss: 0.0077 - accuracy: 0.9967 - val_loss: 0.0026 - val_accuracy: 0.9993
    Epoch 24/50
    898/898 - 6s - loss: 0.0075 - accuracy: 0.9973 - val_loss: 0.0013 - val_accuracy: 0.9998
    Epoch 25/50
    898/898 - 6s - loss: 0.0076 - accuracy: 0.9971 - val_loss: 0.0023 - val_accuracy: 0.9987
    Epoch 26/50
    898/898 - 6s - loss: 0.0081 - accuracy: 0.9967 - val_loss: 0.0021 - val_accuracy: 0.9996
    Epoch 27/50
    898/898 - 6s - loss: 0.0076 - accuracy: 0.9969 - val_loss: 0.0018 - val_accuracy: 0.9996
    Epoch 28/50
    898/898 - 6s - loss: 0.0084 - accuracy: 0.9970 - val_loss: 0.0021 - val_accuracy: 0.9996
    Epoch 29/50
    898/898 - 6s - loss: 0.0067 - accuracy: 0.9973 - val_loss: 0.0015 - val_accuracy: 1.0000
    Epoch 30/50
    898/898 - 6s - loss: 0.0076 - accuracy: 0.9970 - val_loss: 0.0018 - val_accuracy: 0.9996
    Epoch 31/50
    898/898 - 6s - loss: 0.0073 - accuracy: 0.9973 - val_loss: 0.0029 - val_accuracy: 0.9991
    Epoch 32/50
    898/898 - 6s - loss: 0.0063 - accuracy: 0.9981 - val_loss: 0.0015 - val_accuracy: 0.9998
    Epoch 33/50
    898/898 - 6s - loss: 0.0068 - accuracy: 0.9975 - val_loss: 8.3982e-04 - val_accuracy: 1.0000
    Epoch 34/50
    898/898 - 6s - loss: 0.0082 - accuracy: 0.9973 - val_loss: 0.0015 - val_accuracy: 0.9998
    Epoch 35/50
    898/898 - 6s - loss: 0.0068 - accuracy: 0.9973 - val_loss: 0.0054 - val_accuracy: 0.9982
    Epoch 36/50
    898/898 - 6s - loss: 0.0065 - accuracy: 0.9977 - val_loss: 4.9994e-04 - val_accuracy: 1.0000
    Epoch 37/50
    898/898 - 6s - loss: 0.0056 - accuracy: 0.9981 - val_loss: 0.0051 - val_accuracy: 0.9975
    Epoch 38/50
    898/898 - 6s - loss: 0.0072 - accuracy: 0.9977 - val_loss: 0.0024 - val_accuracy: 0.9989
    Epoch 39/50
    898/898 - 6s - loss: 0.0043 - accuracy: 0.9984 - val_loss: 3.3114e-04 - val_accuracy: 1.0000
    Epoch 40/50
    898/898 - 6s - loss: 0.0071 - accuracy: 0.9977 - val_loss: 0.0042 - val_accuracy: 0.9989
    Epoch 41/50
    898/898 - 6s - loss: 0.0052 - accuracy: 0.9982 - val_loss: 0.0012 - val_accuracy: 1.0000
    Epoch 42/50
    898/898 - 6s - loss: 0.0074 - accuracy: 0.9975 - val_loss: 5.8946e-04 - val_accuracy: 1.0000
    Epoch 43/50
    898/898 - 6s - loss: 0.0057 - accuracy: 0.9979 - val_loss: 5.0277e-04 - val_accuracy: 1.0000
    Epoch 44/50
    898/898 - 6s - loss: 0.0057 - accuracy: 0.9979 - val_loss: 0.0011 - val_accuracy: 0.9998
    Epoch 45/50
    898/898 - 6s - loss: 0.0056 - accuracy: 0.9983 - val_loss: 4.1463e-04 - val_accuracy: 1.0000
    Epoch 46/50
    898/898 - 6s - loss: 0.0039 - accuracy: 0.9983 - val_loss: 0.0022 - val_accuracy: 0.9989
    Epoch 47/50
    898/898 - 6s - loss: 0.0068 - accuracy: 0.9974 - val_loss: 0.0014 - val_accuracy: 0.9996
    Epoch 48/50
    898/898 - 6s - loss: 0.0037 - accuracy: 0.9986 - val_loss: 8.7349e-04 - val_accuracy: 0.9998
    Epoch 49/50
    898/898 - 6s - loss: 0.0060 - accuracy: 0.9981 - val_loss: 8.4601e-04 - val_accuracy: 0.9998
    Epoch 50/50
    898/898 - 6s - loss: 0.0075 - accuracy: 0.9975 - val_loss: 0.0027 - val_accuracy: 0.9991
    


```python
plt.plot(history.history["accuracy"])
plt.plot(history.history["val_accuracy"])
```





    
![png](/images/output_30_1.png)
    


# Model Evaluation


```python
test_url = "https://github.com/PhilChodrow/PIC16b/blob/master/datasets/fake_news_test.csv?raw=true"
test_df = pd.read_csv(test_url)
test_data = make_dataset(test_df)
```


```python
final_model.evaluate(test_data)
```

    22449/22449 [==============================] - 75s 3ms/step - loss: 0.1199 - accuracy: 0.9801
    




    [0.11991225183010101, 0.9800881743431091]


Overall, we got an accuracy of 98% on testing data! Great!

# Embedding Visualization


```python
weights = final_model.get_layer('embedding').get_weights()[0] 
vocab = vectorize_layer.get_vocabulary()
weights 
```




    array([[-5.2264710e-03,  5.1196385e-04, -8.6333537e-03, ...,
            -1.1822039e-01, -3.3501282e-03, -1.2406845e-03],
           [ 6.5733254e-02, -1.4222296e-01,  9.8022483e-02, ...,
             4.9006173e-01,  4.7776360e-02, -1.9676579e-02],
           [ 6.1194372e-01, -4.2480612e-01,  7.5135911e-01, ...,
             2.9965830e-01,  5.1205397e-01,  4.6962723e-01],
           ...,
           [-5.7638091e-01,  4.8679489e-01, -7.6278257e-01, ...,
            -7.7116627e-01, -8.1695873e-01, -3.8794503e-01],
           [-1.0361199e+00,  1.8434341e+00, -4.3759751e-01, ...,
            -3.1293232e+00, -4.3180507e-01, -2.3372035e-01],
           [ 5.0356543e-01,  3.7081900e-01,  5.2391285e-01, ...,
            -1.3391870e+00,  9.0177411e-01,  1.0753752e+00]], dtype=float32)




```python
from sklearn.decomposition import PCA
pca = PCA(n_components=2)
weights = pca.fit_transform(weights)

embedding_df = pd.DataFrame({
    'word' : vocab, 
    'x0'   : weights[:,0],
    'x1'   : weights[:,1]
})
```


```python
import plotly.express as px 
fig = px.scatter(embedding_df, 
                 x = "x0", 
                 y = "x1", 
                 size = list(np.ones(len(embedding_df))),
                 size_max = 2,
                 hover_name = "word")

fig.show()
```

{% include blog_post3.html %}