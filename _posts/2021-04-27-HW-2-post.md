---
layout: post
title: Blog Post 2
---

In this blog post, I explore *spectral clustering* which is an algorithm that allows us to flexibly cluster data with no preconceptions about how data might be strucured or formed. This is unlike traditional clustering (i.e. K-Means) which assumes that points in a respective cluster are spherically distributed around a respective cluster's centers.

### Looking at K-Means Clustering


```python
# import relevant projects
import numpy as np
from sklearn import datasets
from matplotlib import pyplot as plt
```


```python
n = 200
# create random sed
np.random.seed(1111)

# generate random data surrounded with two distinct centers
X, y = datasets.make_blobs(n_samples=n, shuffle=True, random_state=None, centers = 2, cluster_std = 2.0)
plt.scatter(X[:,0], X[:,1]);
```


    
![png](/images/output_3_0.png)
    



```python
from sklearn.cluster import KMeans
km = KMeans(n_clusters = 2)

# fit model with two clusters
km.fit(X)

plt.scatter(X[:,0], X[:,1], c = km.predict(X));
```


    
![png](/images/output_4_0.png)
    



```python
# add noise to data to make mishapen
X, y = datasets.make_moons(n_samples=n, shuffle=True, noise=0.05, random_state=None)
plt.scatter(X[:,0], X[:,1]);
```


    
![png](/images/output_5_0.png)
    



```python
km = KMeans(n_clusters = 2)

# fit KMeans to mishapen data
km.fit(X)
plt.scatter(X[:,0], X[:,1], c = km.predict(X));
```


    
![png](/images/output_6_0.png)
    


K-Means has performed poorly. Since the blobs are now crescents, k-means (which looks for circular clusters) is unable to correctly cluster ou data. Now we pivot to using **spherical clustering** to solve the problem.

## Construct a Similarity Matrix

Let us use the distance between points to create clusters. In doing so we can create a matrix with shape `(n, n)` such that `n` is the number of data points. This becomes our similarity matrix. Some aspects of this similarity matrix include that each entry  `A[i,j]` should be equal to `1` if `X[i]` is within distance of some specified `epsilon` of `X[j]`. On top of this, because we don't want to compare points to each other **the diagonal entries `A[i,i]` should all be equal to zero.**

We will use use `epsilon = 0.4` for the sake of consistency.

{::options parse_block_html="true" /}
<div class="gave-help">
I recommended that my peers use pairwise_distances from sklearn to compute all pairwise distances in a more efficient fashion. By doing so they are able to avoid manually searching to see whether `(X[i] - X[j])**2 < epsilon**2` with a less efficient tool like a `for-loop` for example.
</div>
{::options parse_block_html="false" /}

```python
import numpy
# use pairwise_distances from sklearn to compute all the pairwise distances 
from sklearn.metrics.pairwise import pairwise_distances
```


{::options parse_block_html="true" /}
<div class="got-help">
My peers recommended I use `epsilon**2` as opposed to just `epsilon` in order to create better demarcated clusters.
```python
# initialize epsilon
epsilon = 0.4
# default metric = 'euclidean' for pairwise_distances()
# calculate distance between each datapoint in X
A = pairwise_distances(X)
# Ensure entry A[i,j] should be equal to 1 if X[i] is within distance epsilon of X[j] 
A = (A < epsilon**2).astype(int)
# set up diagonal entries A[i,i] to all be equal to zero
np.fill_diagonal(A, 0)
```
</div>
{::options parse_block_html="false" /}

```python
# display
plt.imshow(A);
```


    
![png](/images/output_11_0.png)
    


## Creating the Cut Term

With the creation of our similarity matrix `A` we now have the distance between points. Now we can characterize the clusters we discover. 

Let us use the *binary norm cut objective* of a matrix $$\mathbf{A}$$ to do this:

The *binary norm cut objective* of a matrix $$\mathbf{A}$$ is defined as: 

$$N_{\mathbf{A}}(C_0, C_1)\equiv \mathbf{cut}(C_0, C_1)\left(\frac{1}{\mathbf{vol}(C_0)} + \frac{1}{\mathbf{vol}(C_1)}\right)\;.$$

Such that, 
- $$\mathbf{cut}(C_0, C_1) \equiv \sum_{i \in C_0, j \in C_1} a_{ij}$$ is the *cut* of the clusters $$C_0$$ and $$C_1$$. 
- $$\mathbf{vol}(C_0) \equiv \sum_{i \in C_0}d_i$$, where $$d_i = \sum_{j = 1}^n a_{ij}$$ is the *degree* of row $$i$$ (the total number of all other rows related to row $$i$$ through $$A$$). The *volume* of cluster $$C_0$$ is a measure of the size of the cluster. 

The cut term $$\mathbf{cut}(C_0, C_1)$$tells us the number of nonzero entries in $$\mathbf{A}$$ that "connect" cluster $$C_0$$ to points in cluster $$C_1$$. 

We want this cut value to be as small as possible since this corresponds to good clustering.


```python
def cut(A,y):
    """
    calculate the cut term of two clusters given a similarity matrix A, and y (which specifies cluster membership)
    """
    
    # add entries from the similarity matrix
    cut_term = 0
    
    # go over entries A[i,j] for each pair of points (i,j) in different clusters
    for i in range(len(y)):
        for j in range(len(y)):
            # check if a pair of points are in different clusters, and add entries to cut_term is applicable
            if (y[i] != y[j]):
                cut_term += A[i,j]
    # return cut term
    return cut_term
```

To prove our point above, let us create the cut objective for our true clusters `y` and then create the cut objective for some random labels.


```python
true_cut = cut(A,y)
# generate a random vector of random labels of length n, with each label equal to either 0 or 1
random_y = np.random.randint(2, size = n)
random_cut = cut(A, random_y)
```


```python
# cut objective for the true labels is much smaller than the cut objective for the random labels
# this tells us that the actual clusters are able to categorize data far better than random labels
print("Cut Objective (True Labels):", true_cut, "\nCut Objective (Random Labels):", random_cut)
```

    Cut Objective (True Labels): 0 
    Cut Objective (Random Labels): 746
    

As expected we find that the cut objective for the true labels is far smaller than the cut objective for the random labels. 

## Creating the Volume Term

The 2nd part of the norm cut objective is what is called the *volume term*. The volume term allows us to measure how big clusters. In order to do so we compute the degree in a row - which in turn tells us the number of connections across points. We then can compute the sum for degrees corresponding to a particular cluster (i.e. $$C_0$$ and $$C_1$$).


```python
def vols(A,y):
    """
    calculate the volumne of two clusters given a similarity matrix A, and y (which specifies cluster membership)
    """
    # degree of row i (the total number of all other rows related to row i through A)
    d = A.sum(axis = 1)
    # calculate volume of C0
    v0 = d[y == 0].sum()
    # calculate volume of C1
    v1 = d[y == 1].sum()
    
    # return volume of C0 and C1 as a tuple such that v0 holds the volume of cluster 0 and v1 holds the volume of cluster 1
    return (v0, v1)
```

## Put the Cut Term and Volume Term Together


```python
def normcut(A,y):
    """
    compute the binary normalized cut objective of a matrix A with clustering vector y
    """
    # get volume of cluster 0 (v0) and of cluster 1 (v1)
    v0, v1 = vols(A,y)
    # compute binary normalized cut objective
    return cut(A,y) * ((1/v0) + (1/v1))
```

#### Compare Results


```python
# normcut objective using the true labels y
normcut(A,y)
```




    0.0




```python
# normcut objective using the fake labels y (random_y)
normcut(A,random_y)
```




    1.9754369176967979



After comparing the normcut objective using both the true labels y and the fake labels we generated above, we see that normcut for true labels is smaller than that for fake labels. 

## Optimize

Let us transform our vector to compute: 

$$\mathbf{z} \in \mathbb{R}^n$$ such that: 

$$
z_i = 
\begin{cases}
    \frac{1}{\mathbf{vol}(C_0)} &\quad \text{if } y_i = 0 \\ 
    -\frac{1}{\mathbf{vol}(C_1)} &\quad \text{if } y_i = 1 \\ 
\end{cases}
$$

In doing so we are able to find a cluster vector y such that `normcut(A,y)` is small.


```python
def transform(A,y):
    """
    define a new vector 𝐳∈ℝ𝑛 such that: 𝑧𝑖={1 / 𝐯𝐨𝐥(𝐶0) − 1 / 𝐯𝐨𝐥(𝐶1)}
    """
    # compute volumes for v0 and v1 such v0 holds the volume of cluster 0 and v1 holds the volume of cluster 1
    v0, v1 = vols(A,y)
    
    # create a vector of zeros of shape n x 1
    zeros = np.zeros(n)
    
    # for entries in zeros that have the same index as entries in y (such that y==0) assign entries in zero to v0 
    zeros[y==0] = v0
    # for entries in zeros that have the same index as entries in y (such that y==0) assign entries in zero to v0 
    zeros[y==1] = -v1
    
    # return new vector 'z'
    return np.array(1/zeros)
```

Verify our reults by checking:

$$\mathbf{N}_{\mathbf{A}}(C_0, C_1) = 2\frac{\mathbf{z}^T (\mathbf{D} - \mathbf{A})\mathbf{z}}{\mathbf{z}^T\mathbf{D}\mathbf{z}}\;,$$

where $$\mathbf{D}$$ is the diagonal matrix with nonzero entries $$d_{ii} = d_i$$, and  where $$d_i = \sum_{j = 1}^n a_i$$ is the degree (row-sum) from before.  



```python
# create matrix D with shape n x n
D = np.zeros((n,n))

# Fill the main diagonal of the array D with the ith row sum of A
np.fill_diagonal(D, A.sum(axis = 1))

# calculate 'z'
z = transform(A,y)

# find the best clustering in practical time, even for relatively small data sets. 
a = 2 * (z.T @ (D-A) @ z) / (z.T @ D @ z)

# define a normalized cut objective
b = normcut(A,y)
```


```python
# check if a is close to b
# relate matrix product (a) to normcut objective (b)
np.isclose(a,b)
```




    True



By checking the identity $$\mathbf{z}^T\mathbf{D}\mathbb{1} = 0$$ we can say that $$\mathbf{z}$$ contains approximately the same number of positive and negative entries (i.e. entries > 0, entries < 0). This attribute will come in use down the road when we create our "masks" for clustering.


```python
np.isclose(z.T @ D @ np.ones(n), 0)
```




    True



## Minimize Normcut Objective

Let us code up the following formula using the `orth_obj()` function

$$ R_\mathbf{A}(\mathbf{z})\equiv \frac{\mathbf{z}^T (\mathbf{D} - \mathbf{A})\mathbf{z}}{\mathbf{z}^T\mathbf{D}\mathbf{z}} $$ 


```python
def orth(u, v):
    return (u @ v) / (v @ v) * v

e = np.ones(n) 

d = D @ e

def orth_obj(z):
    z_o = z - orth(z, d)
    return (z_o @ (D - A) @ z_o)/(z_o @ D @ z_o)
```


```python
# import minimize function from scipy.optimize
from scipy.optimize import minimize

# use the minimize function from scipy.optimize to minimize the function orth_obj
z_ = minimize(orth_obj, z)
```

## Verify our results


```python
# extract numpy array from z_ (this variable z_min is our minimizer)
z_min = z_.x

# display results using matplotlib
plt.scatter(X[:,0], X[:,1], c = (z_min < 0))
```




    <matplotlib.collections.PathCollection at 0x277d072d8b0>




    
![png](/images/output_37_1.png)
    


Clearly, clustering is no longer spherical in nature. We are making progress!

## Constructing the Laplacian Matrix


Let us now use eigenvalues and eigenvectors to solve the problem of clustering for the sake of efficiency. 

From the Rayleigh-Ritz Theorem we know that minimizing $$\mathbf{z}$$ must be the solution with smallest eigenvalue of the generalized eigenvalue problem 

$$ (\mathbf{D} - \mathbf{A}) \mathbf{z} = \lambda \mathbf{D}\mathbf{z}\;, \quad \mathbf{z}^T\mathbf{D}\mathbb{1} = 0$$

This is equivalent to:  

$$ \mathbf{D}^{-1}(\mathbf{D} - \mathbf{A}) \mathbf{z} = \lambda \mathbf{z}\;, \quad \mathbf{z}^T\mathbb{1} = 0\;.$$

Let us construct the matrix $$\mathbf{L} = \mathbf{D}^{-1}(\mathbf{D} - \mathbf{A})$$ (i.e. the *Laplacian matrix*). Then let's find the eigenvector corresponding to its second-smallest eigenvalue and save that to some value `z_eig`. This eigenvalue, `z_eig`, will serve as our mask to create two distinct colors in our scatterplot.


```python
# create (normalized) Laplacian matrix of the similarity matrix A
L = np.linalg.inv(D) @ (D - A)

# find eigenvalue, eigenvectors from L
Lambda, U = np.linalg.eig(L)

# sort eigenvalues (default sorting algorithm is 'quicksort')
ix = Lambda.argsort()

# sort eigenvalues and eigenvectors column-wise
Lambda, U = Lambda[ix], U[:,ix]

# extract eigenvector corresponding to second-smallest eigenvalue
z_eig = U[:,1]
```


```python
# display results using matplotlib
plt.scatter(X[:,0], X[:,1], c = z_eig);
```


    
![png](/images/output_41_0.png)
    


This looks very accurate! Let us put all our knowledge from earlier together to create a single cohesive function.

## Putting It All Together

Let us take all our work from previous parts and create a function that does the following in 10 or fewer lines for the sake of efficiency:

- Construct the similarity matrix. 
- Construct the Laplacian matrix. 
- Compute the eigenvector with second-smallest eigenvalue of the Laplacian matrix. 
- Return labels based on this eigenvector. 

Additionally let us demonstrate how successful this new function is by running this function on different sets of data

{::options parse_block_html="true" /}
<div class="got-help">
My peers reminded me to cut down this function so it is at most 10 lines of code for the sake of efficiency. I was able to do so by combining many operations especially when constructing my similarity matrix `A`.
```python
def spectral_clustering(X, epsilon):
    """
    Given input data X and distance threshold (epsilon), perform spectral clustering by
    returning an array of binary labels indicating whether data point i is in group 0 or group 1
    
    X: a numpy array (our input data)
    epsilon: distance threshold
    """
    # construct similarity matrix
    A = (pairwise_distances(X) < epsilon).astype(int)
    np.fill_diagonal(A, 0)
    
    # create the Laplacian matrix
    D = np.zeros((X.shape[0],X.shape[0]))
    np.fill_diagonal(D, A.sum(axis = 1))
    
    # identify the eigenvector with second-smallest eigenvalue of the Laplacian matrix.
    Lambda, U = np.linalg.eig(np.linalg.inv(D) @ (D - A))
    ix = Lambda.argsort()
    Lambda, U = Lambda[ix], U[:,ix]
    z_eig = U[:,1]
    
    # create masks to better demarcate clusters 
    z_eig[z_eig > 0] = 1
    
    # return an array of binary labels
    return z_eig
```
</div>
{::options parse_block_html="false" /}

## What Happens as We Increase the Noise in Our Data?



```python
# plot data with no noise and retain original data 'X' and original epsilon value (0.4) from earlier
plt.scatter(X[:,0], X[:,1], c = spectral_clustering(X, epsilon));
```


    
![png](/images/output_46_0.png)
    



```python
n = 1000
# generate different datasets with make_moons 
# increase noise to 0.01
# set epsilon to be 0.5
X, y = datasets.make_moons(n_samples=n, shuffle=True, noise = 0.01, random_state = None)
plt.scatter(X[:,0], X[:,1], c = spectral_clustering(X, epsilon = 0.5));
```


    
![png](/images/output_47_0.png)
    

```python
# increase noise to 0.1, keep epsilon constant
X, y = datasets.make_moons(n_samples=n, shuffle=True, noise = 0.1, random_state = None)
plt.scatter(X[:,0], X[:,1], c = spectral_clustering(X, epsilon = 0.5));
```


    
![png](/images/output_49_0.png)
    



```python
# increase noise to 0.2, keep epsilon constant
X, y = datasets.make_moons(n_samples=n, shuffle=True, noise = 0.2, random_state = None)
plt.scatter(X[:,0], X[:,1], c = spectral_clustering(X, epsilon = 0.5));
```


    
![png](/images/output_50_0.png)
    



```python
# increase noise to 0.5, keep epsilon constant
X, y = datasets.make_moons(n_samples=n, shuffle=True, noise = 0.5, random_state = None)
plt.scatter(X[:,0], X[:,1], c = spectral_clustering(X, epsilon = 0.5));
```


    
![png](/images/output_51_0.png)
    


### Observations
As we increase the noise, clusters become more and more less-defined and tend to become less 'spectral' and more 'circular' in nature. The `noise` argument in make_moons() represents the standard deviation of Gaussian noise added to the data. As a result when we increase `noise` we increase the standardd eviation of Gaussian noise added to the data. this makes the points spread farther and farther from their respective cluster's means  - which gives the effect of more generalizable clusters.

## Test Our Function on the Bull's Eye Data Set
Let us find the optimal epsilon for which we are able to correctly demarcate clusters in the bull's eye data set. We will do so by testing values of `epsilon` between `0` and `1.0`


```python
n = 1000
# model another data set (bull's eye)
X, y = datasets.make_circles(n_samples=n, shuffle=True, noise=0.05, random_state=None, factor = 0.4)
plt.scatter(X[:,0], X[:,1]);
```


    
![png](/images/output_54_0.png)
    



```python
# Note that k-means fails to demarcate concetric circles
km = KMeans(n_clusters = 2)
# k-means is fit to try and categorize 2 clusters in the bull's eye data
km.fit(X)
plt.scatter(X[:,0], X[:,1], c = km.predict(X));
```


    
![png](/images/output_55_0.png)
    



```python
 plt.scatter(X[:,0], X[:,1], c = spectral_clustering(X, epsilon = 0.2));
```


    
![png](/images/output_56_0.png)
    



```python
plt.scatter(X[:,0], X[:,1], c = spectral_clustering(X, epsilon = 0.4));
```


    
![png](/images/output_57_0.png)
    



```python
plt.scatter(X[:,0], X[:,1], c = spectral_clustering(X, epsilon = 0.6));
```


    
![png](/images/output_58_0.png)
    



```python
plt.scatter(X[:,0], X[:,1], c = spectral_clustering(X, epsilon = 0.8));
```


    
![png](/images/output_59_0.png)
    

{::options parse_block_html="true" /}
<div class="gave-help">
I recommended to my peers that they have a comprehensive summary or note on the observations they make at the end of their blog post to explicity state what epsilon values generally gave us correctly separated rings.
</div>
{::options parse_block_html="false" /}

### Observations
After using the `spectral_clustering()` function on the bull's eye data it appears that for `epsilon = 0.4`, I am able to correctly separate the two rings
